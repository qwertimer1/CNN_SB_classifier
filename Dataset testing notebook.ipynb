{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/'\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\n",
    "def as_bytes(bytes_or_text, encoding='utf-8'):\n",
    "    import six as _six\n",
    "    \n",
    "    \"\"\"Converts either bytes or unicode to `bytes`, using utf-8 encoding for text.\n",
    "    Args:\n",
    "    bytes_or_text: A `bytes`, `str`, or `unicode` object.\n",
    "    encoding: A string indicating the charset for encoding unicode.\n",
    "    Returns:\n",
    "    A `bytes` object.\n",
    "    Raises:\n",
    "    TypeError: If `bytes_or_text` is not a binary or unicode string.\n",
    "    \"\"\"\n",
    "    if isinstance(bytes_or_text, _six.text_type):\n",
    "        return bytes_or_text.encode(encoding)\n",
    "    elif isinstance(bytes_or_text, bytes):\n",
    "        return bytes_or_text\n",
    "    else:\n",
    "        raise TypeError('Expected binary or unicode string, got {}'.format(bytes_or_text,))\n",
    "   \n",
    "\n",
    "\n",
    "def create_image_lists(data_dir, testing_percentage, validation_percentage):\n",
    "    MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 -1\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(\"error\")\n",
    "        return None\n",
    "    result = {}\n",
    "    sub_dirs = [x[0] for x in os.walk(data_dir)]\n",
    "    print(sub_dirs)\n",
    "    is_root_dir = True\n",
    "    for sub_dir in sub_dirs:\n",
    "        if is_root_dir:\n",
    "            is_root_dir = False\n",
    "            continue\n",
    "        extensions = ['jpg', 'jpeg', 'JPG', 'JPEG']\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)\n",
    "        if dir_name == data_dir:\n",
    "            continue\n",
    "        for extension in extensions:\n",
    "            file_glob = os.path.join(data_dir, dir_name, '*' + extension)\n",
    "            file_list.extend(glob.glob(file_glob))\n",
    "        if not file_list:\n",
    "            print(\"No files found\")\n",
    "            continue\n",
    "        if len(file_list) < 20:\n",
    "            print(\"folder has less than 20 images, this may cause issues\")\n",
    "        if len(file_list) > MAX_NUM_IMAGES_PER_CLASS:\n",
    "            print(\"folder has too many images\")\n",
    "        label_name = re.sub(r'[^a-z0-9]+', ' ', dir_name.lower())\n",
    "        training_images = []\n",
    "        testing_images = []\n",
    "        validation_images = []\n",
    "        for file_name in file_list:\n",
    "            base_name = os.path.basename(file_name)\n",
    "            hash_name = re.sub(r'_nohash_.*$', '', file_name)\n",
    "            hash_name_hashed = hashlib.sha1(as_bytes(hash_name)).hexdigest()\n",
    "            percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                                (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n",
    "                               (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n",
    "            if percentage_hash < validation_percentage:\n",
    "                validation_images.append(base_name)\n",
    "            elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(base_name)\n",
    "            else:\n",
    "                training_images.append(base_name)\n",
    "            result[label_name] = {\n",
    "                'dir': dir_name,\n",
    "                'training': training_images,\n",
    "                'testing': testing_images,\n",
    "                'validation': validation_images,\n",
    "            }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/RIPPLES', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/OUTCROPS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/MEGA', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/POCKS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/ROCK', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/SEA_GRASS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/DRAGS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/BLACKS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/FLATS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/SANDS', '/home/tim/Documents/Masters/CNN_SB_classifier/datasets/Seabed Images/ROUGHS']\n"
     ]
    }
   ],
   "source": [
    "result = create_image_lists(data_dir, 20, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['outcrops']['training'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
